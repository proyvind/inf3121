\documentclass[UKenglish]{article}  %% ... or USenglish or norsk
\usepackage[utf8]{inputenc}
\usepackage[T1]{url}

\usepackage[a4paper]{geometry}
\usepackage{listings}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}
\usepackage{caption}
\usepackage{subcaption}

\urlstyle{sf}

\usepackage{tikz,babel,textcomp,csquotes,pgf-umlsd,varioref,graphicx, float}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{ifikompendiumforside}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{array}
\usepackage[font={small,it,sf}]{caption}
\usepackage{longtable}
\usepackage{wrapfig}
\usetikzlibrary{arrows, shadows}

\title{INF4121 Project Assignment 1}
\subtitle{Interpret the metrics offered by the static analyzer}
\author{Per Øyvind Karlsen, Melat Fisseha Tekelmichael}

\begin{document}
\ififorside

\section{Introducion}

\subsection{Objective}
The objective of the assignment is to discover what the source code does and
test it manually. This source code then needs to be analyzed using a static
analyzer tool called “Source Monitor”. The tool will help us obtain some
code metrics and interpret them based on the project under analysis to see its
weak parts of the code. The weak points then need to be improved and measure
it again to see the improved metrics.
The main aim will then be to compare the two interpretations of these metrics. 

\subsection{Project selection}
The Java implementation of Minesweeper was the project we chose to analyze for
our assignment and is available on github:

\url{https://github.com/proyvind/inf3121}

\section{Requirement 1}

\subsection{Brief description}
The project we chose consists of three files: “MineField.java”,
“Ranking.java” and a test program  “Minesweeper.java”.
Each file is constructed with having only one class.
The project is a game called Minesweeper where a player is initially presented
with a grid of undifferentiated squares. Some randomly selected squares,
unknown to the player, are designated to contain mines.
Typically, the size of the grid and the number of mines are set in advance by
the user, either by entering the numbers or selecting from defined skill
levels, depending on the implementation. The number of mines, is equivalent to
1/3 the number of squares, or less.

\url{https://en.wikipedia.org/wiki/Minesweeper_(video_game)}

\subsection{Analysis of testable parts}
In order to test its functionality we have used black-box testing. Here we test
the input and see the output we get. We chose the \textbf{Boundary value analysis}
and \textbf{Equivalence class partitioning} techniques.
We have also used two factors that will help us analyse the testable parts.
\textbf{Observability}: where this shows us the degree of possibility to observe the
test results.
\textbf{Understandability}: to check if the component is self-explaining. 

As the program was refactored in a way to be more easily developed further,
being more flexible in game setups and also had funcionality for setting
our own seed value to get predictable results, we could also implement
tests to simulate whole games as the minefield would be predictable.
This would provide us with \textbf{White-box testing} in the form of
\textbf{Regression testing} that could immediaely discover when any game
logic/behavior were to unintentionally change underways.

\subsubsection{Non-functional tests}
Writing non-functional test for this program wouldn't make much sense as
neither security, performance or stress testing is of any elevance as only
one player may play at a time.

\subsubsection{Test cases}
\includegraphics[width=15cm,height=12cm]{test-cases.png}


\section{Requirement 2}

\subsection{Metrics at project level}
For our use of SourceManager for this project, a few details should be
mentioned to explain the metrics initially measured.
For the SourceMonitor project options, we used modified complexity metric
(count switch/case as only one statement), didn't count blank lines for lines
metric and also ignored header \& footer comment.
The reasoning for this is to ensure complexity metric better reflects
readabiliy of code, while also avoid that lines of code that usually helps
making code easier to read doesn't affect lines metric.
Besides these personal choices affecting the metrics, there was also a bug
in SourceMonitor triggered by the initialization of arrays in the MineField()
constructor that broke the metrics reported for this file. In order to work
around this bug the relevant code was moved into a separate function called by
the constructor.


\includegraphics[height=12cm]{project-metrics-summary-original.png}

The application of metrics interpretations can vary when applied in class, file
or method. For this project, we will use list of metrics. And we here by list
out some of the metrics with their interpretation;

\begin{longtable}[H]{| m{1in} |  m{2in} | m{2in} |}
	\hline
	\textbf{Metric} & \textbf{Description} & \textbf{Explanation}\\
	\hline Percent Lines with Comments & Tells how much of the code that
	are made up of comments. & For this project comments makes up 4.8\% of
	all it's code.\\
	\hline Methods per Class & The number of methods in each class of a
	file. In cases of more than one class it is the average of methods in
	all the classes. & In this case it is 5.33.\\
	\hline Average Statements per Method & The total number of statements
	in all methods divided by the number of methods. & For this project this
	is 10.38.\\
	\hline Maximum complexity & Complexity is simply the number of decision
	structures in a method. Maximum complexity is/are the most complex
	method(s) of a file/class. & In this project it is found to be 12.\\
	\hline Maximum Block Depth & The depth measures the number of blocks
	in a method. The maximum block depth then tells us the deepest methods
	of a class. & For this project it is 7.\\
	\hline Average Block Depth & The total of all enclosed blocks depth amount
	divided by depth. & For this project it is 2.73.\\
	\hline Average Complexity & Calculated by dividing the total of each
	method’s complexity by the number of methods. & For this project it is
	3.38.\\
	\hline
\end{longtable}

\includegraphics[height=8cm]{project-kiviat-metrics-graph-original.png}
The measurement is shown in a kiviat graph. A kiviat graph is a tool that
visually displays a set of metrics that provides easy viewing of multiple
metrics against minimum and maximum thresholds.  All metrics are scaled so
that all maximums are in a common circle and the same for minimums.
The metrics are the radials in the circle. The big circle is the maximum
threshold and the small circle is the minimum. The metrics values that are
with these two circles are the acceptable values that doesn’t need
improvement. If not, it needs improvement.

\subsubsection{Biggest file by the number of lines of code}
The biggest file we have in the project is “MineField.java”. It contains 134 lines of code.

\subsubsection{File with most branches}
“MineField.java” has most percentage of branches in this project. It consists of 34.9\% branches.

\subsubsection{File with most complex code}
"MineField.java" is the file with complex code. It's average complexity of 4.11 is the metric we concluded this from.

\subsection{Metrics at file level}

\subsubsection{File metrics interpretation}

\includegraphics[height=8cm]{Minesweeper_java-kiviat-metrics-graph-original.png}

\textbf{FIXME}: Repetive and merely stating the obvious.

\begin{longtable}[H]{| m{2cm} |  m{10cm} | m{4cm} |}
	\hline
	\textbf{Metric} & \textbf{Interpretation} & \textbf{Compared to the whole project}\\
	\hline
	Comments &
	The graph tells us that 17.6\% of the code is made up by comments.
	Being within the green area (8-20\%) of the graph, this indicates that
	the amount of comments is within the ideal range. &
	Much better than (4.8\%).\\
	\hline
	Methods/Class &
	The graph tells us that there are 3 methods per class.
	Being just outside of the green area of the graph, this indicates
	that this is slighly below the ideal range (4-16). &
	A bit lower than (5.33).\\
	\hline
	Avg. Stmts/Methods &
	The graph tells us that the average number of statements per method is
	12.33.
	This is just outside of the green area of the graph and slightly above
	the ideal range	(6-12). &
	A bit higher than (10.38).\\
	\hline
	Max Complexity &
	The graph tells us that the max complexity is 8.
	This is at the upper bound of the green area of the graph and just
	within the ideal range (2-8). &
	Lower than (12*).\\
	\hline
	Max Depth &
	The graph tells us that the max block depth is at 7.
	This is at the upper bound of the green area of the graph and within
	the ideal range (3-7). &
	Identical to (7).\\
	\hline
	Avg. Depth &
	The graph tells us that the average block depth is at 2.73.
	This is fairly above the upper bound of the green area of the graph
	and outside the ideal range (1.0-2.2) &
	Identical to (2.73).\\
	\hline
	Avg. Complexity &
	The average complexity is at 3.67 and within the ideal range (2.0-4.0) &
	Lower than (3.88*).\\
	\hline
\end{longtable}

\subsubsection{Methods refactoring}
For the methods in this file, I would certainly refactor the code in order
to get cleaner code that's easier to read, decreasing the maintenance burden
and especially with regards to make further development of in the future
easier.

\textbf{FIXME: very clumsely written}

Especially the gameCountinue() method stands out as an obvious candidate for
refactoring due to it having the worst metrics.
A big problem with the method is the size of it. 25 statements in the function
drives up the average statements per method to 12.33, which is higher than the
ideal range (6-12).
A lot of the statements are also conditional, with the resulting complexity
ending up at 8.
In order to improve the metrics, many statements should be moved out into
separate functions. This would reduce the average number of statements per
method, while also reducing the complexity due to so many conditional
statements existing in one function.
The unnecessary use of blocks together with conditional statements also leads
to the depth of the function being unreasonably high.
Most of the braces used to create blocks can simply be removed.
Also despite how the comments metric indicated that the commenting was at an
ideal level, the metric severly misrepresented the actual situation as all
the comments in the file turned out to simply be commented out code that
should be removed, rather than actually having an ideal level of comments
as first being given impression by.
The removal of the comments will however leave file without any comments at
all will however leave file without any comments at all. Despite of this
it's still importan to not start adding comments for ie. describing every
method or to explain code that doesn't need explaining just in order to
meet metric goals.
Code should in general be written in such a self-explanatoy way that adding
comments to explain it shouldn't be needed.
After refactoring the class, the only comments left in this file ended up
being api documenation for what would become a newly implemented Minesweeper()
constructor and a public non-static start function().
Private functions not exported through the API and that's easy enough to
understand how works and the purpose of has not been commented due to lack
of need fo.

\section{Requirement 3}

\subsection{Metrics needed to improve}
From the project level metrics, as illustrated by the Kiviat graph earlier,
the lack of comments, average depth and complexity are metrics are methods
that we need to improve.

 not really documented either, but includes code disabled by being
commented out. This metrics looks fine, but after removing the disabled code, no
comments remain.

\subsection{Changes: Minesweeper.java} First of the file Minesweeper.java has a
lot of unnecessary indentations, which give higher average depth for no use. For
example this part:
\lstinputlisting[language=Java]{code/Minesweeper-object.java}
Minesweeper has been rewritten as a proper object oriented class, improving
readability and the program flow.
Most of Minesweeper class remains static despite of being rewritten
as an object oriented class, while start() is the only non-static and public
function of the class. This is due to motivation for rewriting it as object
oriented class has been for easier maintenance of the code without the intent
of multiple instantions of. The start() function has been made public in order
for instantion in different classes, ie. a separate class for testing rather
than using/playing the program with a specific seed value for PRNG set or
other any other debug/testing functionality.
Also as support for defining custom number of rows and colons has beenee added
to the MineField class, other classes with different values for would be
made possible.

Changes such as reducing duplicated statements has been made to better reuse
code and to ensure that where the same code \& behaviour was duplicated earlier,
the same behaviour is now guaranteed and now maintained in just one place like
as with the init() function. According to the metrics as read by the kiviat
graph, this may not look like a good thing with ie. average statemens per
method actually gets below what's considered as ideal and that the number of
function calls gets increased. Yet knowing that the funcion is static to the
class, this issue will be opimized away as the code will be inlined.

\lstinputlisting[language=Java]{code/Minesweeper-simplify.java}
In the code snippet gameContinue() has been trimmed down to just a
fragment of the size, with everything involving handling of user input
having been split out of and moved to the evalInput() function where a
much more easily readable switch/case (lowering the complexiy metric) takes
care of the input string parsing (with the exception of the reading/passing
of user input from gameContinue() with the necessary conversaion of string
to lower case taking place while being passed as argument, otherwise the
code will regress due to switch/case<string comparision being case sensitive).
The code for dealing with probing of mines/moves has also been moved to a method
of it's own where the excessive (and pointless) use of code blocks has been
addressed, resulting in max \& average depth having been reduced from 6 \% 2.70
to 4 \& 2.12.

\subsubsection{File metric changes for Minesweeper.java}

\begin{figure}[h]
	\begin{subfigure}[b]{0.5\textwidth}
		\caption{Original metrics from Kiviat graph of Minesweeper.java before refactoring}
		\includegraphics[width=\textwidth]{Minesweeper_java-kiviat-metrics-graph-original.png}
		\label{minesweeper.java_original}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\caption{New metrics from Kiviat graph of Minesweeper.java after refactoring}
		\includegraphics[width=\textwidth]{Minesweeper_java-kiviat-metrics-graph-final.png}
		\label{minesweeper.java_final}
	\end{subfigure}
\end{figure}

Figure \ref{minesweeper.java_final} shows that the metrics are all within their
ideal ranges and clearly improves over the original
\ref{minesweepe.java_original}, with the exception of average statements and
complexity being slightly below, which as clarified above is due to not
blindly adhering to ideal metrics, where by own knowledge of language beaviour
and valuing benefits from code reusability in terms of maintenance gains.

\subsubsection{Project level metric changes}

\begin{figure}[h]
	\begin{subfigure}[b]{0.5\textwidth}
		\caption{Original metrics from kiviat graph of the whole project before refactoring}
		\includegraphics[width=\textwidth]{project-kiviat-metrics-graph-original.png}
		\label{project_original}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\caption{New metrics from kiviat graph of the whole project after refactoring}
		\includegraphics[width=\textwidth]{project-kiviat-metrics-graph-final.png}
		\label{project_final}
	\end{subfigure}
\end{figure}

Figure \ref{project_final} shows that for the whole project as well, the same
goals of ideal metrics was succesfull with it being quite consisten with the
file specific metrics above.


\end{document}
